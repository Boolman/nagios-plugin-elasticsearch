#!/usr/bin/python

from nagioscheck import NagiosCheck, PerformanceMetric, Status
import json
import urllib2

HEALTH = {'red':    0,
          'yellow': 1,
          'green':  2}

RED    = HEALTH['red']
YELLOW = HEALTH['yellow']
GREEN  = HEALTH['green']

HEALTH_MAP = {0: 'critical',
              1: 'warning',
              2: 'ok'}

SHARD_STATE = {'UNASSIGNED':   1,
               'INITIALIZING': 2,
               'STARTED':      3,
               'RELOCATING':   4}

class ESShard(object):
    def __init__(self, state):
        self.state = state

class ESIndex(object):
    def __init__(self, name, n_shards, n_replicas):
        self.name = name
        self.n_shards = n_shards
        self.n_replicas = n_replicas

class ESNode(object):
    def __init__(self, name=None, esid=None, attributes={}):
        self.esid = esid
        self.name = name
        self.attributes = attributes

class ElasticSearchCheck(NagiosCheck):
    version = '0.1.1'

    def __init__(self):
        NagiosCheck.__init__(self)

        self.health = HEALTH['green']

        self.add_option('f', 'failure-domain', 'failure_domain', "A "
                        "comma-separated list of ElasticSearch "
                        "attributes that make up your cluster's "
                        "failure domain.  This should be the same list "
                        "of attributes that ElasticSearch's location-"
                        "aware shard allocator has been configured "
                        "with.  If this option is supplied, additional "
                        "checks are carried out to ensure that primary "
                        "and replica shards are not stored in the same "
                        "failure domain.")

        self.add_option('H', 'host', 'host', "Hostname or network "
                        "address to probe.  The ElasticSearch API "
                        "should be listening here.  Defaults to "
                        "'localhost'.")

        self.add_option('p', 'port', 'port', "TCP port to probe.  "
                        "The ElasticSearch API should be listening "
                        "here.  Defaults to 9200.")

    def check(self, opts, args):
        host = getattr(opts, 'host') or "localhost"
        port = int(getattr(opts, 'port') or '9200')

        failure_domain = []
        if (isinstance(opts.failure_domain, str) and
            len(opts.failure_domain) > 0):
            failure_domain.extend(opts.failure_domain.split(","))

        #
        # Data retrieval
        #

        # Request cluster 'health'.  /_cluster/health is like a tl;dr 
        # for /_cluster/state (see below).  There is very little useful 
        # information here.  We are primarily interested in ES' cluster 
        # 'health colour':  a little rating ES gives itself to describe 
        # how much pain it is in.
        es_health = get_json(r'http://%s:%d/_cluster/health' %
                             (host, port))

        their_health = HEALTH[es_health['status'].lower()]

        n_nodes = es_health['number_of_nodes']
        n_dnodes = es_health['number_of_data_nodes']
        n_active_shards = es_health['active_shards']
        n_relocating_shards = es_health['relocating_shards']
        n_initialising_shards = es_health['initializing_shards']
        n_unassigned_shards = es_health['unassigned_shards']
        n_shards = (n_active_shards + n_relocating_shards +
                    n_initialising_shards + n_unassigned_shards)

        # Request cluster 'state'.  This be where all the meat at, yo.  
        # Here, we can see a list of all nodes, indexes, and shards in 
        # the cluster.  This response will also contain a map detailing 
        # where all shards are living at this point in time.
        es_state = get_json(r'http://%s:%d/_cluster/state' %
                            (host, port))

        # Request a bunch of useful numbers that we export as perfdata.  
        # Details like the number of get, search, and indexing 
        # operations come from here.
        es_stats = get_json(r'http://%s:%d/_cluster/nodes/_local/'
                             'stats?all=true' % (host, port))

        myid = es_stats['nodes'].keys()[0]

        #
        # Map construction
        #

        # String all the dumb ES* objects into a bunch of transitive 
        # associations so that we may make some useful assertions about 
        # them.
        esid_node_map       = {} # ESID       : <ESNode>
        index_primary_map   = {} # <ESIndex>  : { 0: <ESShard>, ... }
        name_index_map      = {} # 'bar'      : <ESIndex>
        name_node_map       = {} # 'foo'      : <ESNode>
        node_esid_map       = {} # <ESNode>   : ESID
        node_location_map   = {} # <ESNode>   : ('mars',)
        node_shard_map      = {} # <ESNode>   : [ <ESShard>, ... ]
        primary_replica_map = {} # <ESShard>  : [ <ESShard>, ... ]
        shard_location_map  = {} # <ESShard>  : ('mars',)

        # Build node maps:
        #
        #     - esid_node_map
        #     - name_node_map
        #     - node_esid_map
        #     - node_location_map
        #
        nodes = es_state['nodes']
        for n in nodes:
            name = nodes[n]['name']
            attrs = nodes[n]['attributes']
            node = ESNode(name, n, attrs)

            name_node_map[name] = node
            esid_node_map[n] = node
            node_esid_map[node] = n

            if len(failure_domain) > 0:
                try:
                    loc = tuple(map(lambda a: attrs[a], failure_domain))
                except KeyError, e:
                    missing_attr = e.args[0]
                    raise Status('warning',
                                 ("Node '%s' missing location "
                                  "attribute '%s'" %
                                  (name, missing_attr),))

                node_location_map[node] = loc

        # Build index maps:
        #
        #     - name_index_map
        #
        indices = es_state['metadata']['indices']
        for i in indices:
            idx_stns = indices[i]['settings']
            idx = ESIndex(i,
                          int(idx_stns['index.number_of_shards']),
                          int(idx_stns['index.number_of_replicas']))

            name_index_map[i] = idx

        # Build shard maps:
        #
        #     - index_primary_map
        #     - node_shard_map
        #     - primary_replica_map
        #     - shard_location_map
        #
        for i in name_index_map:
            idx = name_index_map[i]

            if idx not in index_primary_map:
                index_primary_map[idx] = dict(map(lambda n: (n, None),
                                                  range(idx.n_shards)))

            idx_shards = (es_state['routing_table']['indices']
                          [i]['shards'])
            for d in idx_shards:
                primary = None
                replicas = []
                for s in idx_shards[d]:
                    shard = ESShard(SHARD_STATE[s['state'].upper()])

                    if s['primary']:
                        primary = shard
                    else:
                        replicas.append(shard)

                    if s['state'] != 'UNASSIGNED':
                        node = esid_node_map[s['node']]

                        if node not in node_shard_map:
                            node_shard_map[node] = []
                        node_shard_map[node].append(shard)

                        if len(failure_domain) > 0:
                            loc = node_location_map[esid_node_map[s['node']]]
                            shard_location_map[shard] = loc

                index_primary_map[idx][int(d)] = primary

                if primary is not None:
                    primary_replica_map[primary] = replicas

        #
        # Perfdata
        #

        perfdata = []

        def dict2perfdata(base, metrics):
            for metric in metrics:
                if len(metric) == 2:
                    label, path = metric
                    unit = ""
                elif len(metric) > 2:
                    label, path, unit = metric
                else:
                    continue

                keys = path.split(".")

                value = base
                for key in keys:
                    if value is None:
                        break
                    try:
                        value = value[key]
                    except KeyError:
                        value = None
                        break

                if value is not None:
                    metric = PerformanceMetric(label=label,
                                               value=value,
                                               unit=unit)
                    perfdata.append(metric)

        def other2perfdata(metrics):
            for metric in metrics:
                if len(metric) == 2:
                    label, value = metric
                    unit = ""
                elif len(metric) > 2:
                    label, value, unit = metric
                else:
                    continue

                if value is not None:
                    metric = PerformanceMetric(label=label,
                                               value=value,
                                               unit=unit)
                    perfdata.append(metric)

        # Add cluster-wide metrics first.  If you monitor all of your ES 
        # cluster nodes with this plugin, they should all report the 
        # same figures for these labels.  Not ideal, but 'tis better to 
        # graph this data multiple times than not graph it at all.
        metrics = [["cluster_nodes",               n_nodes],
                   ["cluster_data_nodes",          n_dnodes],
                   ["cluster_active_shards",       n_active_shards],
                   ["cluster_relocating_shards",   n_relocating_shards],
                   ["cluster_initialising_shards",
                    n_initialising_shards],
                   ["cluster_unassigned_shards",   n_unassigned_shards],
                   ["cluster_total_shards",        n_shards]]

        other2perfdata(metrics)

        metrics = [["storesize",  'indices.store.size_in_bytes', "B"],
                   ["documents",  'indices.docs.count'],
                   ["index_ops",  'indices.indexing.index_total', "c"],
                   ["index_time", 'indices.indexing.'
                                  'index_time_in_millis', "c"],
                   ["query_ops",  'indices.search.query_total', "c"],
                   ["query_time", 'indices.search.'
                                  'query_time_in_millis', "c"],
                   ["flush_ops",  'indices.flush.total', "c"],
                   ["flush_time", 'indices.flush.'
                                  'total_time_in_millis', "c"]]

        dict2perfdata(es_stats['nodes'][myid], metrics)

        #
        # Assertions
        #

        detail = [] # Collect error messages into this list

        msg = "Monitoring cluster '%s'" % es_health['cluster_name']

        # Assertion:  Each shard has one primary in STARTED state.
        downgraded = False

        for idx_name, idx in name_index_map.iteritems():
            for shard_no in range(idx.n_shards):
                primary = index_primary_map[idx][shard_no]
                if primary is None:
                    downgraded |= self.downgrade_health(RED)
                    detail.append("Index '%s' missing primary on "
                                  "shard %d" % (idx_name, shard_no))
                else:
                    if primary.state != SHARD_STATE['STARTED']:
                        downgraded |= self.downgrade_health(RED)
                        detail.append("Index '%s' primary down on "
                                      "shard %d" % (idx_name, shard_no))

        if downgraded:
            msg = ("One or more indexes are missing primary shards.  "
                   "Use -vv to list them.")

        # Assertion:  Each primary has replicas in STARTED state.
        downgraded = False

        for idx_name, idx in name_index_map.iteritems():
            expect_replicas = idx.n_replicas

            for shard_no in range(idx.n_shards):
                primary = index_primary_map[idx][shard_no]

                if primary is None:
                    continue

                has_replicas = len(primary_replica_map[primary])

                if has_replicas < expect_replicas:
                    downgraded |= self.downgrade_health(YELLOW)
                    detail.append("Index '%s' missing replica on "
                                  "shard %d" % (idx_name, shard_no))

                for replica in primary_replica_map[primary]:
                    if replica.state != SHARD_STATE['STARTED']:
                        downgraded |= self.downgrade_health(YELLOW)
                        detail.append("Index '%s' replica down on "
                                      "shard %d" % (idx_name, shard_no))

        if downgraded:
            msg = ("One or more indexes are missing replica shards.  "
                   "Use -vv to list them.")

        # Assertion:  Replicas are not stored in the same failure domain 
        # as their primary.
        downgraded = False

        if len(failure_domain) > 0:
            for idx_name, idx in name_index_map.iteritems():
                for shard_no in range(idx.n_shards):
                    loc_redundancy = set()
                    vulnerable_shards = set()

                    primary = index_primary_map[idx][shard_no]

                    if primary is None:
                        continue

                    try:
                        loc = shard_location_map[primary]
                    except KeyError:
                        continue
                    loc_redundancy.add(loc)
                    vulnerable_shards.add(primary)

                    for replica in primary_replica_map[primary]:
                        try:
                            loc = shard_location_map[replica]
                        except KeyError:
                            continue
                        loc_redundancy.add(loc)
                        vulnerable_shards.add(replica)

                    # Suppress the problem unless at least one of the 
                    # vulnerable shards is on this data node.
                    my_shards = set(node_shard_map[esid_node_map[myid]])
                    if vulnerable_shards.isdisjoint(my_shards):
                        continue

                    if len(loc_redundancy) == 1:
                        downgraded |= self.downgrade_health(YELLOW)
                        loc = ",".join(list(loc_redundancy)[0])
                        detail.append("Index '%s' shard %d only exists "
                                      "in location '%s'" %
                                      (idx_name, shard_no, loc))

        if downgraded:
            msg = ("One or more index shards are not being replicated "
                   "across failure domains.  Use -vv to list them.")

        # ES detected a problem that we did not.  This should never 
        # happen.  (If it does, you should work out what happened, then 
        # fix this code so that we can detect the problem if it happens 
        # again.)  Obviously, in this case, we cannot provide any useful 
        # output to the operator.
        if their_health < self.health:
            raise Status('critical',
                         ("Cluster reports degraded health: '%s'" %
                          es_health['status'],),
                         perfdata)

        raise Status(HEALTH_MAP[self.health],
                     (msg, None, "%s\n\n%s" % (msg, "\n".join(detail))),
                     perfdata)

    def downgrade_health(self, new_health):
        if new_health < self.health:
            self.health = new_health
            return True
        return False

def get_json(uri):
    try:
        f = urllib2.urlopen(uri)
    except urllib2.HTTPError, e:
        raise Status('unknown', ("API failure",
                                 None,
                                 "API failure:\n\n%s" % str(e)))
    except urllib2.URLError, e:
        # The server could be down; make this CRITICAL.
        raise Status('critical', (e.reason,))

    body = f.read()

    try:
        j = json.loads(body)
    except ValueError:
        raise Status('unknown', ("API returned nonsense",))

    return j

if __name__ == '__main__':
    ElasticSearchCheck().run()
